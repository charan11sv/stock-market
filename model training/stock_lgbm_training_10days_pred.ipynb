{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11162775,"sourceType":"datasetVersion","datasetId":6965606}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport polars as pl\n\n# Read Parquet\ndf = pl.read_parquet(\"/kaggle/input/stock-market-1/final_cleaned.parquet\")\n\nprint(\"loading finished\")\n\n# Write CSV\ndf.write_csv(\"stock_market_1.csv\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for ele in df.columns:\n    print(ele)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"columns = df.columns\n\n# Loop through each column and print its top 5 rows\nfor col in columns:\n    print(f\"Column: {col}\")\n    # Select just this column and print the first 5 rows\n    print(df.select(col).head(5))\n    print()  # Blank line for readability","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip stock_market_1_csv.zip stock_market_1.csv","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Testing on sample of 200 companies","metadata":{}},{"cell_type":"code","source":"import polars as pl\n\n# Read the CSV file\ndf_sam = pl.read_csv(\"/kaggle/input/stock-market-200-sample/top_200_companies.csv\")\n\n# Get column names and their data types\ncolumn_types = [(col, df_sam.schema[col]) for col in df_sam.columns]\n\n# Print column types\nfor col, dtype in column_types:\n    print(f\"{col}: {dtype}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2: Identify unique companies\ncompany_ids = df_sam.select(\"Company_ID\").unique().to_series().to_list()\nprint(f\"üè¢ Found {len(company_ids)} unique companies.\")\n\n# Step 3: Define function to create target columns for one company\ndef create_targets(df_company, n_targets=10):\n    for i in range(1, n_targets + 1):\n        df_company = df_company.with_columns(\n            pl.col(\"Close_x\").shift(-i).alias(f\"close_{i}\")\n        )\n    # Drop last n rows where targets will be null\n    df_company = df_company.slice(0, df_company.height - n_targets)\n    return df_company\n\n# Step 4: Process each company\nprocessed_dfs = []\nprint(\"üõ†Ô∏è Creating target variables and trimming last 10 rows per company...\")\nfor idx, company_id in enumerate(company_ids):\n    df_company = df_sam.filter(pl.col(\"Company_ID\") == company_id).sort(\"Date\")\n    df_company = create_targets(df_company)\n    processed_dfs.append(df_company)\n    if (idx + 1) % 20 == 0 or (idx + 1) == len(company_ids):\n        print(f\"‚úÖ Processed {idx + 1}/{len(company_ids)} companies\")\n\n# Combine processed data\ndf_all = pl.concat(processed_dfs)\nprint(\"üì¶ All companies processed and combined.\")\n\n# Step 5: Split into train/test sets\ntrain_dfs = []\ntest_dfs = []\n\nprint(\"‚úÇÔ∏è Splitting into train/test (80/20) for each company...\")\nfor idx, company_id in enumerate(company_ids):\n    df_company = df_all.filter(pl.col(\"Company_ID\") == company_id).sort(\"Date\")\n    total_rows = df_company.height\n    test_size = int(total_rows * 0.2)\n    train_size = total_rows - test_size\n    \n    train_dfs.append(df_company.slice(0, train_size))\n    test_dfs.append(df_company.slice(train_size, test_size))\n\n    if (idx + 1) % 20 == 0 or (idx + 1) == len(company_ids):\n        print(f\"üìä Split {idx + 1}/{len(company_ids)} companies\")\n\n# Combine all train/test splits\ndf_train = pl.concat(train_dfs)\ndf_test = pl.concat(test_dfs)\n\nprint(f\"‚úÖ Train set size: {df_train.height}\")\nprint(f\"‚úÖ Test set size: {df_test.height}\")\n\n# Final output\nprint(\"üéâ Data preprocessing complete. Ready for modeling!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming df_train is your Polars DataFrame\nfor col in df_test.columns:\n    missing_count = df_train.select(pl.col(col).is_null().sum()).item()\n    print(f\"Column '{col}': {missing_count} missing values\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"(\n    # ---------------- Core parameters ----------------\n    boosting_type='gbdt',               # Type of boosting: 'gbdt' (Gradient Boosting), 'dart', 'goss', etc.\n    objective='regression',            # Task type, e.g., regression, regression_l1, huber, fair\n    metric='rmse',                     # Evaluation metric, e.g., rmse, mae, mse\n    n_estimators=100,                  # Number of boosting rounds (trees)\n    learning_rate=0.1,                 # Shrinks the contribution of each tree (step size shrinkage)\n\n    # ---------------- Tree parameters ----------------\n    num_leaves=63,                     # Max number of leaves per tree; higher = more complex model\n    max_depth=5,                       # Max depth of each tree; limits model complexity\n    min_child_samples=20,              # Minimum number of samples in a leaf node\n    min_child_weight=1e-3,             # Minimum sum of instance weight (hessian) in a leaf\n    min_split_gain=0.0,                # Minimum gain required to make a further partition (regularization)\n\n    # ---------------- Regularization ----------------\n    reg_alpha=0.0,                     # L1 regularization term on weights\n    reg_lambda=0.0,                    # L2 regularization term on weights\n\n    # ---------------- Sampling ----------------\n    subsample=1.0,                     # Fraction of data to be randomly sampled for each tree (row sampling)\n    subsample_freq=0,                 # Frequency of bagging. 0 means disabled\n    colsample_bytree=1.0,             # Fraction of features (columns) to be randomly sampled for each tree\n\n    # ---------------- Advanced sampling ----------------\n    feature_fraction_bynode=1.0,       # Fraction of features used per split (node-level)\n    feature_fraction_seed=42,          # Random seed for feature_fraction\n    bagging_fraction=1.0,              # Same as subsample (alias), used in bagging\n    bagging_seed=42,                   # Random seed for bagging\n\n    # ---------------- Execution ----------------\n    n_jobs=-1,                         # Number of threads to use (-1 means use all cores)\n    random_state=42,                   # Random seed for reproducibility\n    verbosity=-1,                      # Controls verbosity: <0 silent, 0 warnings, >0 all messages\n    force_col_wise=True,               # Forces column-wise histogram building (usually faster for sparse data)\n    device='cpu',                      # 'cpu' or 'gpu' ‚Äì device to train the model on\n    max_bin=255,                       # Max number of bins for discretizing continuous features\n\n    # ---------------- Other advanced options ----------------\n    importance_type='split',           # Feature importance type: 'split' (frequency) or 'gain' (information gain)\n    monotone_constraints=None,         # List to enforce monotonic relationship (e.g., [1, -1, 0])\n    boosting='gbdt',                   # Alias for boosting_type\n    class_weight=None,                 # Used for imbalance handling; None or 'balanced'\n    is_unbalance=False,                # If True, automatically balances classes based on data\n    scale_pos_weight=1.0,              # Used for unbalanced classes (typically for binary classification)\n    force_row_wise=False,              # Force row-wise histogram (can be slower, but sometimes more stable)\n    path_smooth=0.0,                   # Controls smoothing of prediction path (mainly for DART)\n    drop_rate=0.1,                     # Dropout rate for DART boosting\n    skip_drop=0.5,                     # Probability to skip dropping during a boosting iteration (DART)\n    xgboost_dart_mode=False,           # Whether to use xgboost dart mode in DART boosting\n    gpu_use_dp=False,                  # Whether to use double precision on GPU (if device='gpu')\n    boosting_rounds=None,              # Deprecated / alias for n_estimators\n    early_stopping_rounds=None,        # Enables early stopping if no improvement over given rounds\n    callbacks=None,                    # List of callback functions to apply during training\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\n\n# Convert Polars to Pandas (LightGBM + scikit-learn use Pandas/Numpy)\nprint(\"üîÅ Converting Polars DataFrames to Pandas...\")\ndf_train_pd = df_train.to_pandas()\ndf_test_pd = df_test.to_pandas()\n\n# Define target columns\ntarget_cols = [f\"close_{i}\" for i in range(1, 11)]\n\n# Drop unwanted columns\ndrop_cols = ['Date', 'Company_ID', 'COMPANY']\nfeature_cols = [col for col in df_train.columns if col not in target_cols + drop_cols]\n\n# Split features and targets\nX_train = df_train_pd[feature_cols]\ny_train = df_train_pd[target_cols]\n\nX_test_full = df_test_pd[feature_cols]\ny_test_full = df_test_pd[target_cols]\n\n# Split test into validation and final test\nprint(\"üì§ Splitting test data into validation and test...\")\nX_val, X_test, y_val, y_test = train_test_split(\n    X_test_full, y_test_full, test_size=0.5, random_state=42\n)\n\nprint(f\"üìä Train size: {X_train.shape[0]}\")\nprint(f\"üìä Validation size: {X_val.shape[0]}\")\nprint(f\"üìä Test size: {X_test.shape[0]}\")\n\n# Define full parameterized LightGBM regressor\nlgb_regressor = lgb.LGBMRegressor(\n    # Core parameters\n    boosting_type='gbdt',\n    objective='regression',\n    metric='rmse',\n    n_estimators=100,\n    learning_rate=0.1,\n\n    # Tree parameters\n    num_leaves=31,\n    max_depth=5,\n    min_child_samples=20,\n    min_child_weight=1e-3,\n    min_split_gain=0.0,\n\n    # Regularization\n    reg_alpha=0.0,\n    reg_lambda=0.0,\n\n    # Sampling\n    subsample=1.0,\n    subsample_freq=0,\n    colsample_bytree=1.0,\n\n    # Advanced sampling\n    feature_fraction_bynode=1.0,\n    feature_fraction_seed=42,\n    bagging_fraction=1.0,\n    bagging_seed=42,\n\n    # Execution\n    n_jobs= 1,\n    random_state=42,\n    verbosity=-1,\n    force_col_wise=True,\n    device='cpu',\n    max_bin=255,\n\n    # Other advanced options\n    importance_type='split',\n    monotone_constraints=None,\n    boosting='gbdt',\n    class_weight=None,\n    is_unbalance=False,\n    scale_pos_weight=1.0,\n    force_row_wise=False,\n    path_smooth=0.0,\n    drop_rate=0.1,\n    skip_drop=0.5,\n    xgboost_dart_mode=False,\n    gpu_use_dp=False,\n    boosting_rounds=None,\n    early_stopping_rounds=None,\n    callbacks=None,\n)\n# Wrap in MultiOutputRegressor\nprint(\"üß† Training LightGBM Multi-Output Regressor...\")\nmulti_model = MultiOutputRegressor(lgb_regressor)\nmulti_model.fit(X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to calculate Mean Absolute Percentage Error (MAPE) per column\ndef calculate_mape_per_target(y_true, y_pred, target_names):\n    mape_scores = {}\n    for i, col in enumerate(target_names):\n        # Avoid division by zero\n        true_vals = y_true[:, i]\n        pred_vals = y_pred[:, i]\n        percentage_errors = np.abs((true_vals - pred_vals) / np.clip(np.abs(true_vals), 1e-8, None)) * 100\n        mape_scores[col] = np.mean(percentage_errors)\n    return mape_scores\n\n# Define target column names\ntarget_cols = [f\"close_{i}\" for i in range(1, 11)]\n\n# Validation evaluation\nprint(\"üìà Evaluating on validation set (with percentage error)...\")\ny_pred_val = multi_model.predict(X_val)\nval_mape_scores = calculate_mape_per_target(y_val.values, y_pred_val, target_cols)\n\nprint(\"üìä Validation Average Percentage Errors (per target):\")\nfor col, error in val_mape_scores.items():\n    print(f\"  {col}: {error:.2f}%\")\n\n# Test evaluation\nprint(\"üß™ Evaluating on test set (with percentage error)...\")\ny_pred_test = multi_model.predict(X_test)\ntest_mape_scores = calculate_mape_per_target(y_test.values, y_pred_test, target_cols)\n\nprint(\"üìä Test Average Percentage Errors (per target):\")\nfor col, error in test_mape_scores.items():\n    print(f\"  {col}: {error:.2f}%\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"1. (\n    # Core parameters\n    boosting_type='gbdt',\n    objective='regression',\n    metric='rmse',\n    n_estimators=100,\n    learning_rate=0.1,\n\n    # Tree parameters\n    num_leaves=31,\n    max_depth=-1,\n    min_child_samples=20,\n    min_child_weight=1e-3,\n    min_split_gain=0.0,\n\n    # Regularization\n    reg_alpha=0.0,\n    reg_lambda=0.0,\n\n    # Sampling\n    subsample=1.0,\n    subsample_freq=0,\n    colsample_bytree=1.0,\n\n    # Advanced sampling\n    feature_fraction_bynode=1.0,\n    feature_fraction_seed=42,\n    bagging_fraction=1.0,\n    bagging_seed=42,\n\n    # Execution\n    n_jobs=-1,\n    random_state=42,\n    verbosity=-1,\n    force_col_wise=True,\n    device='cpu',\n    max_bin=255,\n\n    # Other advanced options\n    importance_type='split',\n    monotone_constraints=None,\n    boosting='gbdt',\n    class_weight=None,\n    is_unbalance=False,\n    scale_pos_weight=1.0,\n    force_row_wise=False,\n    path_smooth=0.0,\n    drop_rate=0.1,\n    skip_drop=0.5,\n    xgboost_dart_mode=False,\n    gpu_use_dp=False,\n    boosting_rounds=None,\n    early_stopping_rounds=None,\n    callbacks=None,\n)\n------------------------üìà Evaluating on validation set (with percentage error)...\nüìä Validation Average Percentage Errors (per target):\n  close_1: 4.15%\n  close_2: 5.31%\n  close_3: 6.16%\n  close_4: 6.91%\n  close_5: 7.88%\n  close_6: 8.43%\n  close_7: 8.94%\n  close_8: 9.26%\n  close_9: 10.20%\n  close_10: 10.49%\nüß™ Evaluating on test set (with percentage error)...\nüìä Test Average Percentage Errors (per target):\n  close_1: 4.27%\n  close_2: 5.46%\n  close_3: 6.30%\n  close_4: 7.09%\n  close_5: 8.06%\n  close_6: 8.60%\n  close_7: 9.08%\n  close_8: 9.43%\n  close_9: 10.39%\n  close_10: 10.68%\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n2. (\n    # Core parameters\n    boosting_type='gbdt',\n    objective='regression',\n    metric='rmse',\n    n_estimators=1000,\n    learning_rate=0.01,\n\n    # Tree parameters\n    num_leaves=31,\n    max_depth=-1,\n    min_child_samples=20,\n    min_child_weight=1e-3,\n    min_split_gain=0.0,\n\n    # Regularization\n    reg_alpha=0.0,\n    reg_lambda=0.0,\n\n    # Sampling\n    subsample=1.0,\n    subsample_freq=0,\n    colsample_bytree=1.0,\n\n    # Advanced sampling\n    feature_fraction_bynode=1.0,\n    feature_fraction_seed=42,\n    bagging_fraction=1.0,\n    bagging_seed=42,\n\n    # Execution\n    n_jobs=-1,\n    random_state=42,\n    verbosity=-1,\n    force_col_wise=True,\n    device='cpu',\n    max_bin=255,\n\n    # Other advanced options\n    importance_type='split',\n    monotone_constraints=None,\n    boosting='gbdt',\n    class_weight=None,\n    is_unbalance=False,\n    scale_pos_weight=1.0,\n    force_row_wise=False,\n    path_smooth=0.0,\n    drop_rate=0.1,\n    skip_drop=0.5,\n    xgboost_dart_mode=False,\n    gpu_use_dp=False,\n    boosting_rounds=None,\n    early_stopping_rounds=None,\n    callbacks=None,\n)\n--------------------------------------------\n close_1: 4.11%\n  close_2: 5.24%\n  close_3: 6.12%\n  close_4: 6.91%\n  close_5: 7.73%\n  close_6: 8.28%\n  close_7: 8.92%\n  close_8: 9.54%\n  close_9: 9.93%\n  close_10: 10.34%\nüß™ Evaluating on test set (with percentage error)...\nüìä Test Average Percentage Errors (per target):\n  close_1: 4.24%\n  close_2: 5.39%\n  close_3: 6.27%\n  close_4: 7.09%\n  close_5: 7.92%\n  close_6: 8.45%\n  close_7: 9.06%\n  close_8: 9.69%\n  close_9: 10.09%\n  close_10: 10.52%\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n3. (\n    # Core parameters\n    boosting_type='gbdt',\n    objective='regression',\n    metric='rmse',\n    n_estimators=100,\n    learning_rate=0.1,\n\n    # Tree parameters\n    num_leaves=31,\n    max_depth=5,\n    min_child_samples=20,\n    min_child_weight=1e-3,\n    min_split_gain=0.0,\n\n    # Regularization\n    reg_alpha=0.0,\n    reg_lambda=0.0,\n\n    # Sampling\n    subsample=1.0,\n    subsample_freq=0,\n    colsample_bytree=1.0,\n\n    # Advanced sampling\n    feature_fraction_bynode=1.0,\n    feature_fraction_seed=42,\n    bagging_fraction=1.0,\n    bagging_seed=42,\n\n    # Execution\n    n_jobs=-1,\n    random_state=42,\n    verbosity=-1,\n    force_col_wise=True,\n    device='cpu',\n    max_bin=255,\n\n    # Other advanced options\n    importance_type='split',\n    monotone_constraints=None,\n    boosting='gbdt',\n    class_weight=None,\n    is_unbalance=False,\n    scale_pos_weight=1.0,\n    force_row_wise=False,\n    path_smooth=0.0,\n    drop_rate=0.1,\n    skip_drop=0.5,\n    xgboost_dart_mode=False,\n    gpu_use_dp=False,\n    boosting_rounds=None,\n    early_stopping_rounds=None,\n    callbacks=None,\n)----------------------------------------------------------------\nclose_1: 3.63%\n  close_2: 4.75%\n  close_3: 5.42%\n  close_4: 6.38%\n  close_5: 7.15%\n  close_6: 7.86%\n  close_7: 8.62%\n  close_8: 9.18%\n  close_9: 9.50%\n  close_10: 9.86%\nüß™ Evaluating on test set (with percentage error)...\nüìä Test Average Percentage Errors (per target):\n  close_1: 3.59%\n  close_2: 4.71%\n  close_3: 5.42%\n  close_4: 6.37%\n  close_5: 7.13%\n  close_6: 7.84%\n  close_7: 8.65%\n  close_8: 9.19%\n  close_9: 9.45%\n  close_10: 9.79%\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n4. (\n    # Core parameters\n    boosting_type='gbdt',\n    objective='regression',\n    metric='rmse',\n    n_estimators=100,\n    learning_rate=0.1,\n\n    # Tree parameters\n    num_leaves=31,\n    max_depth=5,\n    min_child_samples=20,\n    min_child_weight=1e-3,\n    min_split_gain=0.0,\n\n    # Regularization\n    reg_alpha=0.0,\n    reg_lambda=0.0,\n\n    # Sampling\n    subsample=0.6,\n    subsample_freq=0,\n    colsample_bytree=0.3,\n\n    # Advanced sampling\n    feature_fraction_bynode=1.0,\n    feature_fraction_seed=42,\n    bagging_fraction=1.0,\n    bagging_seed=42,\n\n    # Execution\n    n_jobs= 1,\n    random_state=42,\n    verbosity=-1,\n    force_col_wise=True,\n    device='cpu',\n    max_bin=255,\n\n    # Other advanced options\n    importance_type='split',\n    monotone_constraints=None,\n    boosting='gbdt',\n    class_weight=None,\n    is_unbalance=False,\n    scale_pos_weight=1.0,\n    force_row_wise=False,\n    path_smooth=0.0,\n    drop_rate=0.1,\n    skip_drop=0.5,\n    xgboost_dart_mode=False,\n    gpu_use_dp=False,\n    boosting_rounds=None,\n    early_stopping_rounds=None,\n    callbacks=None,\n)-------------------------------------------------------------------\nclose_1: 3.84%\n  close_2: 4.77%\n  close_3: 5.48%\n  close_4: 6.51%\n  close_5: 7.21%\n  close_6: 8.16%\n  close_7: 8.72%\n  close_8: 8.83%\n  close_9: 9.46%\n  close_10: 9.84%\nüß™ Evaluating on test set (with percentage error)...\nüìä Test Average Percentage Errors (per target):\n  close_1: 3.97%\n  close_2: 4.95%\n  close_3: 5.68%\n  close_4: 6.77%\n  close_5: 7.63%\n  close_6: 8.65%\n  close_7: 9.28%\n  close_8: 9.15%\n  close_9: 9.80%\n  close_10: 10.17%\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n5. (\n    # Core parameters\n    boosting_type='gbdt',\n    objective='regression',\n    metric='rmse',\n    n_estimators=100,\n    learning_rate=0.1,\n\n    # Tree parameters\n    num_leaves=31,\n    max_depth=5,\n    min_child_samples=20,\n    min_child_weight=1e-3,\n    min_split_gain=0.0,\n\n    # Regularization\n    reg_alpha=0.0,\n    reg_lambda=0.0,\n\n    # Sampling\n    subsample=0.1,\n    subsample_freq=0,\n    colsample_bytree=0.1,\n\n    # Advanced sampling\n    feature_fraction_bynode=1.0,\n    feature_fraction_seed=42,\n    bagging_fraction=1.0,\n    bagging_seed=42,\n\n    # Execution\n    n_jobs= 1,\n    random_state=42,\n    verbosity=-1,\n    force_col_wise=True,\n    device='cpu',\n    max_bin=255,\n\n    # Other advanced options\n    importance_type='split',\n    monotone_constraints=None,\n    boosting='gbdt',\n    class_weight=None,\n    is_unbalance=False,\n    scale_pos_weight=1.0,\n    force_row_wise=False,\n    path_smooth=0.0,\n    drop_rate=0.1,\n    skip_drop=0.5,\n    xgboost_dart_mode=False,\n    gpu_use_dp=False,\n    boosting_rounds=None,\n    early_stopping_rounds=None,\n    callbacks=None,\n)--------------------------------------------------------------------------------\nclose_1: 4.94%\n  close_2: 5.68%\n  close_3: 6.46%\n  close_4: 6.98%\n  close_5: 7.73%\n  close_6: 8.07%\n  close_7: 8.69%\n  close_8: 8.91%\n  close_9: 9.54%\n  close_10: 9.63%\nüß™ Evaluating on test set (with percentage error)...\nüìä Test Average Percentage Errors (per target):\n  close_1: 5.07%\n  close_2: 5.77%\n  close_3: 6.59%\n  close_4: 7.10%\n  close_5: 7.89%\n  close_6: 8.18%\n  close_7: 8.88%\n  close_8: 9.01%\n  close_9: 9.65%\n  close_10: 9.79%\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import lightgbm\nimport sklearn\nfrom sklearn.multioutput import MultiOutputRegressor\n\nprint(\"LightGBM version:\", lightgbm.__version__)\nprint(\"Scikit-learn version:\", sklearn.__version__)\nprint(\"MultiOutputRegressor module location:\", MultiOutputRegressor.__module__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T13:47:23.821035Z","iopub.execute_input":"2025-03-26T13:47:23.821397Z","iopub.status.idle":"2025-03-26T13:47:23.827660Z","shell.execute_reply.started":"2025-03-26T13:47:23.821370Z","shell.execute_reply":"2025-03-26T13:47:23.826818Z"}},"outputs":[{"name":"stdout","text":"LightGBM version: 4.5.0\nScikit-learn version: 1.2.2\nMultiOutputRegressor module location: sklearn.multioutput\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### creating train test for full data","metadata":{}},{"cell_type":"code","source":"import polars as pl\n\n# Read the Parquet file\ndf1 = pl.read_parquet(\"/kaggle/input/stock-market-1/final_cleaned.parquet\")\n\n# Get column names and their data types\ncolumn_types = [(col, df1.schema[col]) for col in df1.columns]\n\n# Print column types\nfor col, dtype in column_types:\n    print(f\"{col}: {dtype}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2: Identify unique companies\ncompany_ids = df1.select(\"Company_ID\").unique().to_series().to_list()\nprint(f\"üè¢ Found {len(company_ids)} unique companies.\")\n\n# Step 3: Define function to create target columns for one company\ndef create_targets(df_company, n_targets=10):\n    for i in range(1, n_targets + 1):\n        df_company = df_company.with_columns(\n            pl.col(\"Close_x\").shift(-i).alias(f\"close_{i}\")\n        )\n    # Drop last n rows where targets will be null\n    df_company = df_company.slice(0, df_company.height - n_targets)\n    return df_company\n\n# Step 4: Process each company\nprocessed_dfs = []\nprint(\"üõ†Ô∏è Creating target variables and trimming last 10 rows per company...\")\nfor idx, company_id in enumerate(company_ids):\n    df_company = df1.filter(pl.col(\"Company_ID\") == company_id).sort(\"Date\")\n    df_company = create_targets(df_company)\n    processed_dfs.append(df_company)\n    if (idx + 1) % 20 == 0 or (idx + 1) == len(company_ids):\n        print(f\"‚úÖ Processed {idx + 1}/{len(company_ids)} companies\")\n\n# Combine processed data\ndf_all = pl.concat(processed_dfs)\nprint(\"üì¶ All companies processed and combined.\")\n\n# Step 5: Split into train/test sets\ntrain_dfs = []\ntest_dfs = []\n\nprint(\"‚úÇÔ∏è Splitting into train/test (80/20) for each company...\")\nfor idx, company_id in enumerate(company_ids):\n    df_company = df_all.filter(pl.col(\"Company_ID\") == company_id).sort(\"Date\")\n    total_rows = df_company.height\n    test_size = int(total_rows * 0.2)\n    train_size = total_rows - test_size\n    \n    train_dfs.append(df_company.slice(0, train_size))\n    test_dfs.append(df_company.slice(train_size, test_size))\n\n    if (idx + 1) % 20 == 0 or (idx + 1) == len(company_ids):\n        print(f\"üìä Split {idx + 1}/{len(company_ids)} companies\")\n\n# Combine all train/test splits\ndf_train = pl.concat(train_dfs)\ndf_test = pl.concat(test_dfs)\n\nprint(f\"‚úÖ Train set size: {df_train.height}\")\nprint(f\"‚úÖ Test set size: {df_test.height}\")\n\n# Final output\nprint(\"üéâ Data preprocessing complete. Ready for modeling!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del df1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del company_ids, create_targets,processed_dfs, df_all","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del train_dfs, test_dfs,idx, company_id","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save to Parquet\ndf_train.write_parquet(\"/kaggle/working/stock_df_train.parquet\")\ndf_test.write_parquet(\"/kaggle/working/stock_df_test.parquet\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink, display \n\n# Display download links\nprint(\"Download links:\")\ndisplay(FileLink(\"stock_df_train.parquet\"))\ndisplay(FileLink(\"stock_df_test.parquet\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training with full data","metadata":{}},{"cell_type":"code","source":"import polars as pl\n\n# Load the saved Parquet files\ndf_test = pl.read_parquet(\"/kaggle/input/stock-final-train-test/stock_df_test.parquet\")\ndf_train = pl.read_parquet(\"/kaggle/input/stock-final-train-test/stock_df_train.parquet\")\n\n# Quick sanity check\nprint(\"‚úÖ Loaded DataFrames:\")\nprint(f\"üì¶ Train shape: {df_train.shape}\")\nprint(f\"üì¶ Test shape: {df_test.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T11:55:06.243050Z","iopub.execute_input":"2025-03-26T11:55:06.243337Z","iopub.status.idle":"2025-03-26T11:55:19.702128Z","shell.execute_reply.started":"2025-03-26T11:55:06.243312Z","shell.execute_reply":"2025-03-26T11:55:19.701289Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Loaded DataFrames:\nüì¶ Train shape: (5112156, 101)\nüì¶ Test shape: (1277054, 101)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\nimport time\n\n# Convert Polars to Pandas\nprint(\"üîÅ Converting Polars DataFrames to Pandas...\")\ndf_train_pd = df_train.to_pandas()\ndf_test_pd = df_test.to_pandas()\n\n# Define target columns\ntarget_cols = [f\"close_{i}\" for i in range(1, 11)]\n\n# Drop unwanted columns\ndrop_cols = ['Date', 'Company_ID', 'COMPANY']\nfeature_cols = [col for col in df_train.columns if col not in target_cols + drop_cols]\n\n# Split features and targets\nX_train = df_train_pd[feature_cols]\ny_train = df_train_pd[target_cols]\n\nX_test_full = df_test_pd[feature_cols]\ny_test_full = df_test_pd[target_cols]\n\n# Validation split\nprint(\"üì§ Splitting test data into validation and test...\")\nX_val, X_test, y_val, y_test = train_test_split(\n    X_test_full, y_test_full, test_size=0.5, random_state=42\n)\n\nprint(f\"üìä Train size: {X_train.shape[0]}\")\nprint(f\"üìä Validation size: {X_val.shape[0]}\")\nprint(f\"üìä Test size: {X_test.shape[0]}\")\n\n# Start timing\nstart_time = time.time()\n\n# Use GPU for LightGBM\nlgb_regressor = lgb.LGBMRegressor(\n    # Core parameters\n    boosting_type='gbdt',\n    objective='regression',\n    metric='rmse',\n    n_estimators=7000,\n    learning_rate=0.1,\n\n    # Tree parameters\n    num_leaves=31,\n    max_depth=20,\n    min_child_samples=20,\n    min_child_weight=1e-3,\n    min_split_gain=0.0,\n\n    # Regularization\n    reg_alpha=0.0,\n    reg_lambda=0.0,\n\n    # Sampling\n    subsample=1.0,\n    subsample_freq=0,\n    colsample_bytree=1.0,\n\n    # Advanced sampling\n    feature_fraction_bynode=1.0,\n    feature_fraction_seed=42,\n    bagging_fraction=1.0,\n    bagging_seed=42,\n\n    # Execution\n    random_state=42,\n    verbosity=-1,\n    force_col_wise=True,\n    n_jobs=-1,                     # Use all CPU cores for preprocessing\n    device='gpu',                  # üî• Use GPU\n    gpu_platform_id=0,\n    gpu_device_id=0,\n    max_bin=255,\n\n    # Other advanced options\n    importance_type='split',\n    monotone_constraints=None,\n    boosting='gbdt',\n    class_weight=None,\n    is_unbalance=False,\n    scale_pos_weight=1.0,\n    force_row_wise=False,\n    path_smooth=0.0,\n    drop_rate=0.1,\n    skip_drop=0.5,\n    \n    xgboost_dart_mode=False,\n    gpu_use_dp=False,\n    boosting_rounds=None,\n    early_stopping_rounds=None,\n    callbacks=None,\n)\n# (\n#     boosting_type='gbdt',\n#     objective='regression',\n#     metric='rmse',\n#     n_estimators=1000,             # More trees, GPU can handle it\n#     learning_rate=0.05,\n#     num_leaves=64,\n#     max_depth=-1,\n#     subsample=0.8,\n#     colsample_bytree=0.8,\n#     random_state=42,\n#     n_jobs=-1,                     # Use all CPU cores for preprocessing\n#     device='gpu',                  # üî• Use GPU\n#     gpu_platform_id=0,\n#     gpu_device_id=0,\n#     verbosity=-1,\n# )\n\n# Wrap with MultiOutput\nprint(\"‚ö° Training LightGBM Multi-Output Regressor using GPU...\")\nmulti_model = MultiOutputRegressor(lgb_regressor)\n\nmulti_model.fit(X_train, y_train)\n\n# End timing\nend_time = time.time()\nprint(f\"‚è±Ô∏è Training completed in {end_time - start_time:.2f} seconds.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T11:56:08.905327Z","iopub.execute_input":"2025-03-26T11:56:08.905681Z","iopub.status.idle":"2025-03-26T12:58:33.566539Z","shell.execute_reply.started":"2025-03-26T11:56:08.905653Z","shell.execute_reply":"2025-03-26T12:58:33.565573Z"}},"outputs":[{"name":"stdout","text":"üîÅ Converting Polars DataFrames to Pandas...\nüì§ Splitting test data into validation and test...\nüìä Train size: 5112156\nüìä Validation size: 638527\nüìä Test size: 638527\n‚ö° Training LightGBM Multi-Output Regressor using GPU...\n‚è±Ô∏è Training completed in 3727.73 seconds.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# # Function to calculate Mean Absolute Percentage Error (MAPE) per column\n# def calculate_mape_per_target(y_true, y_pred, target_names):\n#     mape_scores = {}\n#     for i, col in enumerate(target_names):\n#         # Avoid division by zero\n#         true_vals = y_true[:, i]\n#         pred_vals = y_pred[:, i]\n#         percentage_errors = np.abs((true_vals - pred_vals) / np.clip(np.abs(true_vals), 1e-8, None)) * 100\n#         mape_scores[col] = np.mean(percentage_errors)\n#     return mape_scores\n\n# # Define target column names\n# target_cols = [f\"close_{i}\" for i in range(1, 11)]\n\n# # Validation evaluation\n# print(\"üìà Evaluating on validation set (with percentage error)...\")\n# y_pred_val = multi_model.predict(X_val)\n# val_mape_scores = calculate_mape_per_target(y_val.values, y_pred_val, target_cols)\n\n# print(\"üìä Validation Average Percentage Errors (per target):\")\n# for col, error in val_mape_scores.items():\n#     print(f\"  {col}: {error:.2f}%\")\n\n# # Test evaluation\n# print(\"üß™ Evaluating on test set (with percentage error)...\")\n# y_pred_test = multi_model.predict(X_test)\n# test_mape_scores = calculate_mape_per_target(y_test.values, y_pred_test, target_cols)\n\n# print(\"üìä Test Average Percentage Errors (per target):\")\n# for col, error in test_mape_scores.items():\n#     print(f\"  {col}: {error:.2f}%\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cupy as cp  # Use CuPy instead of NumPy\n\n# GPU-accelerated MAPE calculation\ndef calculate_mape_per_target_gpu(y_true, y_pred, target_names):\n    mape_scores = {}\n    y_true_gpu = cp.asarray(y_true)\n    y_pred_gpu = cp.asarray(y_pred)\n    \n    for i, col in enumerate(target_names):\n        true_vals = y_true_gpu[:, i]\n        pred_vals = y_pred_gpu[:, i]\n        # Avoid division by zero\n        percentage_errors = cp.abs((true_vals - pred_vals) / cp.clip(cp.abs(true_vals), 1e-8, None)) * 100\n        mape_scores[col] = cp.mean(percentage_errors).item()  # .item() to convert back to float\n    return mape_scores\n\n# Define target column names\ntarget_cols = [f\"close_{i}\" for i in range(1, 11)]\n\n# Validation evaluation\nprint(\"üìà Evaluating on validation set (with percentage error, GPU accelerated)...\")\ny_pred_val = multi_model.predict(X_val)\nval_mape_scores = calculate_mape_per_target_gpu(y_val.values, y_pred_val, target_cols)\n\nprint(\"üìä Validation Average Percentage Errors (per target):\")\nfor col, error in val_mape_scores.items():\n    print(f\"  {col}: {error:.2f}%\")\n\n# Test evaluation\nprint(\"üß™ Evaluating on test set (with percentage error, GPU accelerated)...\")\ny_pred_test = multi_model.predict(X_test)\ntest_mape_scores = calculate_mape_per_target_gpu(y_test.values, y_pred_test, target_cols)\n\nprint(\"üìä Test Average Percentage Errors (per target):\")\nfor col, error in test_mape_scores.items():\n    print(f\"  {col}: {error:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T12:58:33.567629Z","iopub.execute_input":"2025-03-26T12:58:33.568244Z","iopub.status.idle":"2025-03-26T13:30:26.882529Z","shell.execute_reply.started":"2025-03-26T12:58:33.568219Z","shell.execute_reply":"2025-03-26T13:30:26.881673Z"}},"outputs":[{"name":"stdout","text":"üìà Evaluating on validation set (with percentage error, GPU accelerated)...\nüìä Validation Average Percentage Errors (per target):\n  close_1: 5.60%\n  close_2: 8.46%\n  close_3: 10.28%\n  close_4: 10.88%\n  close_5: 12.47%\n  close_6: 13.68%\n  close_7: 14.17%\n  close_8: 14.62%\n  close_9: 15.49%\n  close_10: 17.36%\nüß™ Evaluating on test set (with percentage error, GPU accelerated)...\nüìä Test Average Percentage Errors (per target):\n  close_1: 5.87%\n  close_2: 8.82%\n  close_3: 10.75%\n  close_4: 10.84%\n  close_5: 12.63%\n  close_6: 13.83%\n  close_7: 14.35%\n  close_8: 14.68%\n  close_9: 15.57%\n  close_10: 17.37%\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"close_1: 9.87%\n  close_2: 13.36%\n  close_3: 15.78%\n  close_4: 17.85%\n  close_5: 18.71%\n  close_6: 19.90%\n  close_7: 21.61%\n  close_8: 22.01%\n  close_9: 22.45%\n  close_10: 23.46%\n----------------------------\n(10, 2000)\nüìà Evaluating on validation set (with percentage error, GPU accelerated)...\nüìä Validation Average Percentage Errors (per target):\n  close_1: 6.82%\n  close_2: 8.60%\n  close_3: 11.41%\n  close_4: 11.71%\n  close_5: 13.72%\n  close_6: 13.98%\n  close_7: 16.38%\n  close_8: 16.98%\n  close_9: 18.48%\n  close_10: 18.49%\nüß™ Evaluating on test set (with percentage error, GPU accelerated)...\nüìä Test Average Percentage Errors (per target):\n  close_1: 6.98%\n  close_2: 8.76%\n  close_3: 11.24%\n  close_4: 11.62%\n  close_5: 13.80%\n  close_6: 13.67%\n  close_7: 16.22%\n  close_8: 16.94%\n  close_9: 18.19%\n  close_10: 18.28%\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n(15, 2000)\nclose_1: 6.16%\n  close_2: 8.16%\n  close_3: 9.76%\n  close_4: 11.41%\n  close_5: 12.48%\n  close_6: 14.17%\n  close_7: 15.65%\n  close_8: 16.55%\n  close_9: 18.37%\n  close_10: 17.32%\nüß™ Evaluating on test set (with percentage error, GPU accelerated)...\nüìä Test Average Percentage Errors (per target):\n  close_1: 7.02%\n  close_2: 8.56%\n  close_3: 10.00%\n  close_4: 11.06%\n  close_5: 12.50%\n  close_6: 14.13%\n  close_7: 15.47%\n  close_8: 16.35%\n  close_9: 17.78%\n  close_10: 17.08%\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n(15, 4000)\nclose_1: 5.84%\n  close_2: 8.16%\n  close_3: 10.44%\n  close_4: 11.49%\n  close_5: 13.14%\n  close_6: 14.38%\n  close_7: 15.72%\n  close_8: 16.20%\n  close_9: 17.69%\n  close_10: 16.73%\nüß™ Evaluating on test set (with percentage error, GPU accelerated)...\nüìä Test Average Percentage Errors (per target):\n  close_1: 6.85%\n  close_2: 8.52%\n  close_3: 10.92%\n  close_4: 11.14%\n  close_5: 13.29%\n  close_6: 14.58%\n  close_7: 15.69%\n  close_8: 16.11%\n  close_9: 17.16%\n  close_10: 16.58%\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++==","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import joblib\njoblib.dump(multi_model, 'multi_model_stock1.pkl')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T13:43:14.903911Z","iopub.execute_input":"2025-03-26T13:43:14.904281Z","iopub.status.idle":"2025-03-26T13:43:19.238670Z","shell.execute_reply.started":"2025-03-26T13:43:14.904250Z","shell.execute_reply":"2025-03-26T13:43:19.237931Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"['multi_model_stock1.pkl']"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cupy as cp\nprint(cp.__version__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"import polars as pl\n\n# Load the saved Parquet files\ndf_test = pl.read_parquet(\"/kaggle/input/stock-final-train-test/stock_df_test.parquet\")\ndf_train = pl.read_parquet(\"/kaggle/input/stock-final-train-test/stock_df_train.parquet\")\n\n# Quick sanity check\nprint(\"‚úÖ Loaded DataFrames:\")\nprint(f\"üì¶ Train shape: {df_train.shape}\")\nprint(f\"üì¶ Test shape: {df_test.shape}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport optuna\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport time","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming you already have df_train as Polars and converted it to df_train_pd\ntarget_cols = [f\"close_{i}\" for i in range(1, 11)]\ndrop_cols = ['Date', 'Company_ID', 'COMPANY']\nfeature_cols = [col for col in df_train.columns if col not in target_cols + drop_cols]\n\nX = df_train[feature_cols]\ny = df_train[target_cols]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_mape_per_target(y_true, y_pred, target_names):\n    mape_scores = {}\n    for i, col in enumerate(target_names):\n        true_vals = y_true[:, i]\n        pred_vals = y_pred[:, i]\n        percentage_errors = np.abs((true_vals - pred_vals) / np.clip(np.abs(true_vals), 1e-8, None)) * 100\n        mape_scores[col] = np.mean(percentage_errors)\n    return mape_scores\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Store MAPE scores from each trial for plotting\n# trial_mape_history = {}\n\n# def objective(trial):\n#     params = {\n#         'objective': 'regression',\n#         'metric': 'rmse',\n#         'boosting_type': 'gbdt',\n#         'device': 'gpu',\n#         'verbosity': -1,\n#         'learning_rate': 0.1,\n#         'num_leaves': trial.suggest_int(\"num_leaves\", 31, 45),\n#         'max_depth': trial.suggest_int(\"max_depth\", 5, 10, 15),\n#         'subsample': trial.suggest_float(\"subsample\", 0.6, 1.0),\n#         'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.3, 1.0),\n#         # 'reg_alpha': trial.suggest_float(\"reg_alpha\", 0.0, 0.05),\n#         # 'reg_lambda': trial.suggest_float(\"reg_lambda\", 0.0, 0.05),\n#         'n_estimators': trial.suggest_int( \"n_estimators\", 100, 150, 200)\n#     }\n\n#     kf = KFold(n_splits= 5, shuffle=False)  # shuffle=False for time series\n#     rmses = []\n#     mape_scores = {col: [] for col in target_cols}\n\n#     for train_idx, val_idx in kf.split(X):\n#         X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n#         y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n#         preds = []\n#         true_vals = []\n\n#         for i, col in enumerate(target_cols):\n#             model = lgb.LGBMRegressor(**params)\n#             model.fit(X_train, y_train[col])\n#             pred = model.predict(X_val)\n#             preds.append(pred)\n#             true_vals.append(y_val[col].values)\n\n#         preds = np.stack(preds, axis=1)\n#         true_vals = np.stack(true_vals, axis=1)\n\n#         # RMSE for Optuna\n#         rmse = mean_squared_error(true_vals, preds, squared=False)\n#         rmses.append(rmse)\n\n#         # MAPE tracking\n#         fold_mape = calculate_mape_per_target(true_vals, preds, target_cols)\n#         for col in target_cols:\n#             mape_scores[col].append(fold_mape[col])\n\n#     # Save average MAPE per target for this trial\n#     avg_mape_per_target = {col: np.mean(mape_scores[col]) for col in target_cols}\n#     trial_mape_history[trial.number] = avg_mape_per_target\n\n#     return np.mean(rmses)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Store MAPE scores from each trial for plotting\ntrial_mape_history = {}\n\ndef objective(trial):\n    # Suggested parameters with corrected `step=...`\n    params = {\n        'objective': 'regression',\n        'metric': 'rmse',\n        'boosting_type': 'gbdt',\n        'device': 'gpu',\n        'verbosity': -1,\n        'learning_rate': 0.1,\n        'max_depth': trial.suggest_int(\"max_depth\", 5, 15, step=5),\n        'num_leaves': trial.suggest_categorical(\"num_leaves\", [31, 45]),\n        'subsample': 1.0,\n        'colsample_bytree': 1.0,\n        'n_estimators': trial.suggest_int(\"n_estimators\", 1000, 8000, step=500)\n    }\n\n    kf = KFold(n_splits=5, shuffle=False)\n    rmses = []\n    mape_scores = {col: [] for col in target_cols}\n\n    for train_idx, val_idx in kf.split(X):\n        # Polars slicing + convert to Pandas\n        X_train = X[train_idx].to_pandas()\n        X_val = X[val_idx].to_pandas()\n        y_train = y[train_idx].to_pandas()\n        y_val = y[val_idx].to_pandas()\n\n        preds = []\n        true_vals = []\n\n        for col in target_cols:\n            model = lgb.LGBMRegressor(**params)\n            model.fit(X_train, y_train[col])\n            pred = model.predict(X_val)\n            preds.append(pred)\n            true_vals.append(y_val[col].values)\n\n        preds = np.stack(preds, axis=1)\n        true_vals = np.stack(true_vals, axis=1)\n\n        # RMSE for Optuna optimization\n        rmse = mean_squared_error(true_vals, preds, squared=False)\n        rmses.append(rmse)\n\n        # MAPE tracking\n        fold_mape = calculate_mape_per_target(true_vals, preds, target_cols)\n        for col in target_cols:\n            mape_scores[col].append(fold_mape[col])\n\n    avg_mape_per_target = {col: np.mean(mape_scores[col]) for col in target_cols}\n    trial_mape_history[trial.number] = avg_mape_per_target\n\n    # ‚úÖ Clean and concise trial info\n    print(f\"\\nTrial {trial.number}\")\n    print(f\"Params: {params}\")\n    print(f\"Final RMSE: {np.mean(rmses):.4f}\")\n    print(f\"Avg MAPE per target:\")\n    for col in target_cols:\n        print(f\"  {col}: {avg_mape_per_target[col]:.2f}%\")\n    print(\"-\" * 40)\n\n    return np.mean(rmses)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_time = time.time()\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=20)  # You can increase this to 50+\n\nend_time = time.time()\nprint(f\"‚è±Ô∏è Hyperparameter tuning completed in {end_time - start_time:.2f} seconds.\")\nprint(\"‚úÖ Best Trial:\", study.best_trial.params)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot MAPE per target for each trial\nplt.figure(figsize=(14, 7))\nfor trial_num, mape_scores in trial_mape_history.items():\n    values = [mape_scores[col] for col in target_cols]\n    plt.plot(target_cols, values, label=f'Trial {trial_num}')\n\nplt.title(\"MAPE per Target Variable across Trials\")\nplt.ylabel(\"MAPE (%)\")\nplt.xlabel(\"Target Variable\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
