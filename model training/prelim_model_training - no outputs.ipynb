{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28d6b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your full dataset\n",
    "file_path = 'final_cleaned.parquet'\n",
    "df = pd.read_parquet(file_path)\n",
    "print(df.columns)\n",
    "# Extract a small portion of the dataset (e.g., first 100 rows)\n",
    "sample_df = df.head(100)  # Adjust the number of rows as needed\n",
    "\n",
    "# Save the sample dataset to a CSV file\n",
    "sample_file_path = 'sample_dataset.csv'\n",
    "sample_df.to_csv(sample_file_path, index=False)\n",
    "\n",
    "print(f'Sample dataset saved as {sample_file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad54c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Remove multicollinearity using VIF\n",
    "def remove_high_vif_features(X, threshold=5.0):\n",
    "    X_numeric = X.select_dtypes(include=[np.number])  # Select only numeric columns\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data['feature'] = X_numeric.columns\n",
    "    vif_data['VIF'] = [variance_inflation_factor(X_numeric.values, i) for i in range(X_numeric.shape[1])]\n",
    "    \n",
    "    # Remove features with VIF higher than the threshold\n",
    "    high_vif_features = vif_data[vif_data['VIF'] > threshold]['feature']\n",
    "    return X.drop(columns=high_vif_features)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "def apply_pca(X_train, X_val, n_components=0.95):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_val_pca = pca.transform(X_val)\n",
    "    return pd.DataFrame(X_train_pca), pd.DataFrame(X_val_pca)\n",
    "\n",
    "# Aggressive feature selection using correlation and RFE\n",
    "def select_important_features(X, y, num_features):\n",
    "    # Remove highly correlated features\n",
    "    corr_matrix = X.corr().abs()\n",
    "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]\n",
    "    X_reduced = X.drop(columns=to_drop)\n",
    "\n",
    "    # Recursive feature elimination (RFE) with linear regression\n",
    "    model = LinearRegression()\n",
    "    rfe = RFE(model, n_features_to_select=num_features)\n",
    "    X_rfe_reduced = rfe.fit_transform(X_reduced, y)\n",
    "\n",
    "    return pd.DataFrame(X_rfe_reduced)\n",
    "\n",
    "# Simplified Transformer model definition\n",
    "class StockTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, nhead=2):  # Reduce nhead to simplify architecture\n",
    "        super(StockTransformer, self).__init__()\n",
    "        self.nhead = nhead\n",
    "\n",
    "        # Check if input_dim is divisible by nhead, if not adjust nhead\n",
    "        if input_dim % nhead != 0:\n",
    "            self.nhead = 1  # Adjust to 1 head if not divisible\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=self.nhead, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=2)  # Reduce number of layers\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=1)  # Reduce sequence length dimension\n",
    "        return self.fc(self.dropout(x))\n",
    "\n",
    "# Train Transformer\n",
    "def train_transformer(X_train, y_train, X_val, y_val, input_dim, device='cpu'):\n",
    "    model = StockTransformer(input_dim).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Add a sequence dimension (unsqueeze to add a dimension for sequence length)\n",
    "    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    for epoch in range(20000):  # Reduce the number of epochs to avoid overfitting\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train_tensor)\n",
    "        loss = criterion(output, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_output = model(X_val_tensor)\n",
    "            val_loss = criterion(val_output, y_val_tensor)\n",
    "        print(f'Epoch {epoch + 1}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Preprocessing function with KNN Imputation and scaling\n",
    "def preprocess_data(X):\n",
    "    # Select only numeric columns\n",
    "    X_numeric = X.select_dtypes(include=[np.number])\n",
    "\n",
    "    # Replace infinity values with NaN\n",
    "    X_numeric.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # Cap extreme large values (based on domain knowledge)\n",
    "    X_numeric = X_numeric.clip(lower=-1e6, upper=1e6)\n",
    "    \n",
    "    # Use KNN imputation for handling missing values\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    X_numeric = pd.DataFrame(imputer.fit_transform(X_numeric), columns=X_numeric.columns)\n",
    "    \n",
    "    return X_numeric\n",
    "\n",
    "# Process batches for LightGBM and Transformer for two companies\n",
    "def process_two_companies(data, target_column, batch_size=2, num_features=10, device='cpu'):\n",
    "    # Select any two companies for processing\n",
    "    unique_companies = data['Company_ID'].unique()[:2]  # Selecting only two companies\n",
    "    \n",
    "    for i in range(0, len(unique_companies), batch_size):\n",
    "        batch_companies = unique_companies[i:i+batch_size]\n",
    "        batch_data = data[data['Company_ID'].isin(batch_companies)]\n",
    "        \n",
    "        X_train = batch_data.drop(columns=[target_column, 'Company_ID', 'Date'])\n",
    "        y_train = batch_data[target_column]\n",
    "\n",
    "        # Preprocess the data (handles both internal and external features)\n",
    "        X_train = preprocess_data(X_train)\n",
    "\n",
    "        # Scaling the data\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "        # Remove multicollinearity using VIF\n",
    "        X_train_vif_reduced = remove_high_vif_features(pd.DataFrame(X_train_scaled, columns=X_train.columns))\n",
    "        \n",
    "        # Aggressive feature selection (correlation and RFE)\n",
    "        X_train_reduced = select_important_features(X_train_vif_reduced, y_train, num_features)\n",
    "\n",
    "        # Train LightGBM model with reduced features\n",
    "        lgb_model = LGBMRegressor(n_estimators=100)\n",
    "        lgb_model.fit(X_train_reduced, y_train)\n",
    "        y_pred_lgb = lgb_model.predict(X_train_reduced)\n",
    "\n",
    "        # Train Transformer model\n",
    "        input_dim = X_train_reduced.shape[1]\n",
    "        transformer_model = train_transformer(X_train_reduced, y_train, X_train_reduced, y_train, input_dim, device=device)\n",
    "\n",
    "        # Get predictions for Transformer\n",
    "        X_train_tensor = torch.tensor(X_train_reduced.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "        y_pred_transformer = transformer_model(X_train_tensor).detach().cpu().numpy()\n",
    "\n",
    "        # Ensemble prediction\n",
    "        ensemble_pred = (y_pred_lgb * 0.9 + y_pred_transformer.squeeze() * 0.1)\n",
    "\n",
    "        # Calculate RMSE\n",
    "        print(f'RMSE for LightGBM: {mean_squared_error(y_train, y_pred_lgb, squared=False)}')\n",
    "        print(f'RMSE for Transformer: {mean_squared_error(y_train, y_pred_transformer.squeeze(), squared=False)}')\n",
    "        print(f'RMSE for Ensemble: {mean_squared_error(y_train, ensemble_pred, squared=False)}')\n",
    "\n",
    "# Main function\n",
    "def train_model_on_small_dataset(file_path, target_column='Close_x', batch_size=200, num_features=100):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    process_two_companies(df, target_column, batch_size, num_features)\n",
    "\n",
    "# Assuming your dataset is saved as 'final_cleaned.parquet'\n",
    "file_path = 'final_cleaned.parquet'\n",
    "train_model_on_small_dataset(file_path, num_features=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee358707",
   "metadata": {},
   "outputs": [],
   "source": [
    "## previous 200 epochs, 0.01 lr\n",
    "## curr-previous 3000 epochs, 0.001 lr, num features 10, batches 200 -----RMSE for LightGBM: 8.597191087745076, RMSE for Transformer: 17.6809587444915, RMSE for Ensemble: 8.558077688024794\n",
    "## ## curr-previous 20000 epochs, 0.001 lr, num features 100,batches 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2061b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Remove multicollinearity using VIF\n",
    "def remove_high_vif_features(X, threshold=5.0):\n",
    "    X_numeric = X.select_dtypes(include=[np.number])\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data['feature'] = X_numeric.columns\n",
    "    vif_data['VIF'] = [variance_inflation_factor(X_numeric.values, i) for i in range(X_numeric.shape[1])]\n",
    "\n",
    "    high_vif_features = vif_data[vif_data['VIF'] > threshold]['feature']\n",
    "    return X.drop(columns=high_vif_features)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "def apply_pca(X_train, X_val, n_components=0.95):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_val_pca = pca.transform(X_val)\n",
    "    return pd.DataFrame(X_train_pca), pd.DataFrame(X_val_pca)\n",
    "\n",
    "# Aggressive feature selection using correlation and RFE\n",
    "def select_important_features(X, y, num_features):\n",
    "    corr_matrix = X.corr().abs()\n",
    "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]\n",
    "    X_reduced = X.drop(columns=to_drop)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    rfe = RFE(model, n_features_to_select=num_features)\n",
    "    X_rfe_reduced = rfe.fit_transform(X_reduced, y)\n",
    "\n",
    "    return pd.DataFrame(X_rfe_reduced)\n",
    "\n",
    "# Simplified Transformer model definition\n",
    "class StockTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, nhead=2):\n",
    "        super(StockTransformer, self).__init__()\n",
    "        self.nhead = nhead\n",
    "        if input_dim % nhead != 0:\n",
    "            self.nhead = 1\n",
    "\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=self.nhead, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.fc(self.dropout(x))\n",
    "\n",
    "# Train Transformer\n",
    "def train_transformer(X_train, y_train, X_val, y_val, input_dim, epochs, learning_rate, model=None, device='cpu'):\n",
    "    if model is None:\n",
    "        model = StockTransformer(input_dim).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train_tensor)\n",
    "        loss = criterion(output, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_output = model(X_val_tensor)\n",
    "            val_loss = criterion(val_output, y_val_tensor)\n",
    "        print(f'Epoch {epoch + 1}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Preprocessing function with KNN Imputation and scaling\n",
    "def preprocess_data(X):\n",
    "    X_numeric = X.select_dtypes(include=[np.number])\n",
    "    X_numeric.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    X_numeric = X_numeric.clip(lower=-1e6, upper=1e6)\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    X_numeric = pd.DataFrame(imputer.fit_transform(X_numeric), columns=X_numeric.columns)\n",
    "    return X_numeric\n",
    "\n",
    "# Save intermediate model\n",
    "def save_model(model, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f'Model saved as {filename}')\n",
    "\n",
    "# Load saved model\n",
    "def load_model(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    print(f'Model {filename} loaded successfully.')\n",
    "    return model\n",
    "\n",
    "# Process batches for LightGBM and Transformer for two companies\n",
    "def process_two_companies(data, target_column, batch_size=2, num_features=10, device='cpu'):\n",
    "    unique_companies = data['Company_ID'].unique()[:2]\n",
    "\n",
    "    for i in range(0, len(unique_companies), batch_size):\n",
    "        batch_companies = unique_companies[i:i + batch_size]\n",
    "        batch_data = data[data['Company_ID'].isin(batch_companies)]\n",
    "\n",
    "        X_train = batch_data.drop(columns=[target_column, 'Company_ID', 'Date'])\n",
    "        y_train = batch_data[target_column]\n",
    "\n",
    "        X_train = preprocess_data(X_train)\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "        X_train_vif_reduced = remove_high_vif_features(pd.DataFrame(X_train_scaled, columns=X_train.columns))\n",
    "        X_train_reduced = select_important_features(X_train_vif_reduced, y_train, num_features)\n",
    "\n",
    "        train_lgbm = input(\"Do you want to train LightGBM? (y/n): \").lower() == 'y'\n",
    "        if train_lgbm:\n",
    "            num_estimators = int(input(\"Enter the number of estimators for LightGBM: \"))\n",
    "            learning_rate = float(input(\"Enter the learning rate for LightGBM: \"))\n",
    "            max_depth = int(input(\"Enter the max depth for LightGBM: \"))\n",
    "            num_leaves = int(input(\"Enter the number of leaves for LightGBM: \"))\n",
    "            feature_fraction = float(input(\"Enter the feature fraction for LightGBM (0.0 to 1.0): \"))\n",
    "            bagging_fraction = float(input(\"Enter the bagging fraction for LightGBM (0.0 to 1.0): \"))\n",
    "            bagging_freq = int(input(\"Enter the bagging frequency for LightGBM: \"))\n",
    "            lambda_l1 = float(input(\"Enter the L1 regularization for LightGBM: \"))\n",
    "            lambda_l2 = float(input(\"Enter the L2 regularization for LightGBM: \"))\n",
    "\n",
    "            lgb_model = LGBMRegressor(\n",
    "                n_estimators=num_estimators,\n",
    "                learning_rate=learning_rate,\n",
    "                max_depth=max_depth,\n",
    "                num_leaves=num_leaves,\n",
    "                feature_fraction=feature_fraction,\n",
    "                bagging_fraction=bagging_fraction,\n",
    "                bagging_freq=bagging_freq,\n",
    "                lambda_l1=lambda_l1,\n",
    "                lambda_l2=lambda_l2\n",
    "            )\n",
    "\n",
    "            lgb_model.fit(X_train_reduced, y_train)\n",
    "            y_pred_lgb = lgb_model.predict(X_train_reduced)\n",
    "            print(f'RMSE for LightGBM: {mean_squared_error(y_train, y_pred_lgb, squared=False)}')\n",
    "\n",
    "            save_lgbm = input(\"Do you want to save this LightGBM model? (y/n): \").lower() == 'y'\n",
    "            if save_lgbm:\n",
    "                filename = input(\"Enter the filename to save LightGBM model: \")\n",
    "                save_model(lgb_model, filename)\n",
    "\n",
    "        train_transformer = input(\"Do you want to train the Transformer? (y/n): \").lower() == 'y'\n",
    "        if train_transformer:\n",
    "            epochs = int(input(\"Enter the number of epochs for Transformer: \"))\n",
    "            transformer_lr = float(input(\"Enter the learning rate for Transformer: \"))\n",
    "\n",
    "            transformer_model = None\n",
    "            load_transformer = input(\"Do you want to load a saved Transformer model? (y/n): \").lower() == 'y'\n",
    "            if load_transformer:\n",
    "                filename = input(\"Enter the filename of the saved Transformer model: \")\n",
    "                transformer_model = load_model(filename)\n",
    "\n",
    "            transformer_model = train_transformer(X_train_reduced, y_train, X_train_reduced, y_train, X_train_reduced.shape[1], epochs, transformer_lr, model=transformer_model, device=device)\n",
    "            \n",
    "            save_transformer = input(\"Do you want to save this Transformer model? (y/n): \").lower() == 'y'\n",
    "            if save_transformer:\n",
    "                filename = input(\"Enter the filename to save Transformer model: \")\n",
    "                save_model(transformer_model, filename)\n",
    "\n",
    "        continue_training = input(\"Do you want to continue training the models? (y/n): \").lower() == 'y'\n",
    "        if continue_training:\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "\n",
    "# Main function\n",
    "def train_model_on_small_dataset(file_path, target_column='Close_x', batch_size=2, num_features=10):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    process_two_companies(df, target_column, batch_size, num_features)\n",
    "\n",
    "# Assuming your dataset is saved as 'final_cleaned.parquet'\n",
    "file_path = 'final_cleaned.parquet'\n",
    "train_model_on_small_dataset(file_path, num_features=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960d90ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import joblib\n",
    "\n",
    "# Simplified Transformer model definition\n",
    "class StockTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, nhead=2):\n",
    "        super(StockTransformer, self).__init__()\n",
    "        self.nhead = nhead\n",
    "        if input_dim % nhead != 0:\n",
    "            self.nhead = 1\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=self.nhead, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.fc(self.dropout(x))\n",
    "\n",
    "# Function to train the transformer model\n",
    "def train_transformer(X_train, y_train, X_val, y_val, input_dim, epochs, learning_rate, model=None, device='cpu'):\n",
    "    if model is None:\n",
    "        model = StockTransformer(input_dim).to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train_tensor)\n",
    "        loss = criterion(output, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_output = model(X_val_tensor)\n",
    "            val_loss = criterion(val_output, y_val_tensor)\n",
    "        print(f'Epoch {epoch + 1}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Function to save transformer models\n",
    "def save_model(model, filename):\n",
    "    torch.save(model.state_dict(), filename)\n",
    "\n",
    "# Function to load transformer models\n",
    "def load_model(filename, input_dim, device='cpu'):\n",
    "    model = StockTransformer(input_dim)\n",
    "    model.load_state_dict(torch.load(filename, map_location=device))\n",
    "    return model\n",
    "\n",
    "# Feature selection function\n",
    "def select_important_features(X, y, num_features):\n",
    "    model = LinearRegression()\n",
    "    rfe = RFE(model, n_features_to_select=num_features)\n",
    "    X_rfe_reduced = rfe.fit_transform(X, y)\n",
    "    return pd.DataFrame(X_rfe_reduced)\n",
    "\n",
    "# LightGBM Training and Save Model Function\n",
    "def train_lightgbm(X_train_reduced, y_train, num_estimators, learning_rate, max_depth, num_leaves, feature_fraction, bagging_fraction, bagging_freq, lambda_l1, lambda_l2):\n",
    "    lgb_model = LGBMRegressor(\n",
    "        n_estimators=num_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "        num_leaves=num_leaves,\n",
    "        feature_fraction=feature_fraction,\n",
    "        bagging_fraction=bagging_fraction,\n",
    "        bagging_freq=bagging_freq,\n",
    "        lambda_l1=lambda_l1,\n",
    "        lambda_l2=lambda_l2\n",
    "    )\n",
    "    \n",
    "    lgb_model.fit(X_train_reduced, y_train)\n",
    "    y_pred_lgb = lgb_model.predict(X_train_reduced)\n",
    "    print(f'RMSE for LightGBM: {mean_squared_error(y_train, y_pred_lgb, squared=False)}')\n",
    "    \n",
    "    save_lgbm = input(\"Do you want to save this LightGBM model? (y/n): \").lower() == 'y'\n",
    "    if save_lgbm:\n",
    "        lgbm_filename = input(\"Enter the filename to save LightGBM model: \")\n",
    "        joblib.dump(lgb_model, f\"{lgbm_filename}.pkl\")\n",
    "        print(f'Model saved as {lgbm_filename}')\n",
    "    \n",
    "    return lgb_model\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_data(X):\n",
    "    X_numeric = X.select_dtypes(include=[np.number])\n",
    "    X_numeric.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    X_numeric.fillna(X_numeric.mean(), inplace=True)\n",
    "    return X_numeric\n",
    "\n",
    "# Main training loop for both LightGBM and Transformer models\n",
    "def process_two_companies(data, target_column, batch_size=2, num_features=10, device='cpu'):\n",
    "    unique_companies = data['Company_ID'].unique()[:2]\n",
    "    \n",
    "    for i in range(0, len(unique_companies), batch_size):\n",
    "        batch_companies = unique_companies[i:i+batch_size]\n",
    "        batch_data = data[data['Company_ID'].isin(batch_companies)]\n",
    "        \n",
    "        X_train = batch_data.drop(columns=[target_column, 'Company_ID', 'Date'])\n",
    "        y_train = batch_data[target_column]\n",
    "\n",
    "        # Preprocess and scale data\n",
    "        X_train = preprocess_data(X_train)\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_train_reduced = select_important_features(pd.DataFrame(X_train_scaled), y_train, num_features)\n",
    "\n",
    "        continue_training = True\n",
    "        while continue_training:\n",
    "            # LightGBM Training\n",
    "            train_lgbm = input(\"Do you want to train LightGBM? (y/n): \").lower() == 'y'\n",
    "            if train_lgbm:\n",
    "                num_estimators = int(input(\"Enter the number of estimators for LightGBM: \"))\n",
    "                learning_rate = float(input(\"Enter the learning rate for LightGBM: \"))\n",
    "                max_depth = int(input(\"Enter the max depth for LightGBM: \"))\n",
    "                num_leaves = int(input(\"Enter the number of leaves for LightGBM: \"))\n",
    "                feature_fraction = float(input(\"Enter the feature fraction for LightGBM (0.0 to 1.0): \"))\n",
    "                bagging_fraction = float(input(\"Enter the bagging fraction for LightGBM (0.0 to 1.0): \"))\n",
    "                bagging_freq = int(input(\"Enter the bagging frequency for LightGBM: \"))\n",
    "                lambda_l1 = float(input(\"Enter the L1 regularization for LightGBM: \"))\n",
    "                lambda_l2 = float(input(\"Enter the L2 regularization for LightGBM: \"))\n",
    "                \n",
    "                lgb_model = train_lightgbm(X_train_reduced, y_train, num_estimators, learning_rate, max_depth, num_leaves, feature_fraction, bagging_fraction, bagging_freq, lambda_l1, lambda_l2)\n",
    "\n",
    "            # Transformer Training\n",
    "            train_transformer_model = input(\"Do you want to train the Transformer? (y/n): \").lower() == 'y'\n",
    "            transformer_model = None\n",
    "            if train_transformer_model:\n",
    "                epochs = int(input(\"Enter the number of epochs for Transformer: \"))\n",
    "                transformer_lr = float(input(\"Enter the learning rate for Transformer: \"))\n",
    "                \n",
    "                load_transformer = input(\"Do you want to load a saved Transformer model? (y/n): \").lower() == 'y'\n",
    "                if load_transformer:\n",
    "                    filename = input(\"Enter the filename of the saved Transformer model: \")\n",
    "                    transformer_model = load_model(filename, X_train_reduced.shape[1], device)\n",
    "                \n",
    "                transformer_model = train_transformer(\n",
    "                    X_train_reduced, y_train, \n",
    "                    X_train_reduced, y_train, \n",
    "                    X_train_reduced.shape[1], \n",
    "                    epochs, transformer_lr, model=transformer_model, device=device\n",
    "                )\n",
    "                \n",
    "                save_transformer = input(\"Do you want to save this Transformer model? (y/n): \").lower() == 'y'\n",
    "                if save_transformer:\n",
    "                    transformer_filename = input(\"Enter the filename to save Transformer model: \")\n",
    "                    save_model(transformer_model, f\"{transformer_filename}.pth\")\n",
    "                    print(f'Transformer model saved as {transformer_filename}')\n",
    "\n",
    "            continue_training = input(\"Do you want to continue training the models? (y/n): \").lower() == 'y'\n",
    "\n",
    "# Main function\n",
    "def train_model_on_small_dataset(file_path, target_column='Close_x', batch_size=2, num_features=10):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    process_two_companies(df, target_column, batch_size, num_features)\n",
    "\n",
    "# Example usage\n",
    "file_path = 'final_cleaned.parquet'\n",
    "train_model_on_small_dataset(file_path, num_features=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1a59ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import joblib\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import concurrent.futures\n",
    "\n",
    "# Simplified Transformer model definition\n",
    "class StockTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, nhead=2):\n",
    "        super(StockTransformer, self).__init__()\n",
    "        self.nhead = nhead\n",
    "        if input_dim % nhead != 0:\n",
    "            self.nhead = 1\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=self.nhead, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.fc(self.dropout(x))\n",
    "\n",
    "# Function to train the transformer model with batch processing\n",
    "def train_transformer(X_train, y_train, X_val, y_val, input_dim, epochs, learning_rate, model=None, batch_size=64, device='cpu'):\n",
    "    if model is None:\n",
    "        model = StockTransformer(input_dim).to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    # Create DataLoader for batching\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch.unsqueeze(1))  # Add sequence dimension\n",
    "            loss = criterion(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_output = model(X_val_tensor.unsqueeze(1))\n",
    "            val_loss = criterion(val_output, y_val_tensor)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Training Loss: {epoch_loss/len(train_loader)}, Validation Loss: {val_loss.item()}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Function to save transformer models\n",
    "def save_model(model, filename):\n",
    "    torch.save(model.state_dict(), filename)\n",
    "\n",
    "# Function to load transformer models\n",
    "def load_model(filename, input_dim, device='cpu'):\n",
    "    model = StockTransformer(input_dim)\n",
    "    model.load_state_dict(torch.load(filename, map_location=device))\n",
    "    return model\n",
    "\n",
    "# Feature selection function\n",
    "def select_important_features(X, y, num_features):\n",
    "    model = LinearRegression()\n",
    "    rfe = RFE(model, n_features_to_select=num_features)\n",
    "    X_rfe_reduced = rfe.fit_transform(X, y)\n",
    "    return pd.DataFrame(X_rfe_reduced)\n",
    "\n",
    "# LightGBM Training Function\n",
    "def train_lightgbm(X_train_reduced, y_train, num_estimators, learning_rate, max_depth, num_leaves, feature_fraction, bagging_fraction, bagging_freq, lambda_l1, lambda_l2):\n",
    "    lgb_model = LGBMRegressor(\n",
    "        n_estimators=num_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "        num_leaves=num_leaves,\n",
    "        feature_fraction=feature_fraction,\n",
    "        bagging_fraction=bagging_fraction,\n",
    "        bagging_freq=bagging_freq,\n",
    "        lambda_l1=lambda_l1,\n",
    "        lambda_l2=lambda_l2\n",
    "    )\n",
    "    \n",
    "    lgb_model.fit(X_train_reduced, y_train)\n",
    "    y_pred_lgb = lgb_model.predict(X_train_reduced)\n",
    "    print(f'RMSE for LightGBM: {mean_squared_error(y_train, y_pred_lgb, squared=False)}')\n",
    "    \n",
    "    save_lgbm = input(\"Do you want to save this LightGBM model? (y/n): \").lower() == 'y'\n",
    "    if save_lgbm:\n",
    "        lgbm_filename = input(\"Enter the filename to save LightGBM model: \")\n",
    "        joblib.dump(lgb_model, f\"{lgbm_filename}.pkl\")\n",
    "        print(f'Model saved as {lgbm_filename}')\n",
    "    \n",
    "    return lgb_model\n",
    "\n",
    "# Parallel LightGBM training function\n",
    "def parallel_lightgbm_training(params_list, X_train_reduced, y_train):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(train_lightgbm, **params) for params in params_list]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                future.result()  # This will raise any exception caught during training\n",
    "            except Exception as exc:\n",
    "                print(f\"Generated an exception: {exc}\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_data(X):\n",
    "    X_numeric = X.select_dtypes(include=[np.number])\n",
    "    X_numeric.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    X_numeric.fillna(X_numeric.mean(), inplace=True)\n",
    "    return X_numeric\n",
    "\n",
    "# Main training loop for both LightGBM and Transformer models\n",
    "def process_two_companies(data, target_column, batch_size=2, num_features=10, device='cpu'):\n",
    "    unique_companies = data['Company_ID'].unique()[:2]\n",
    "    \n",
    "    for i in range(0, len(unique_companies), batch_size):\n",
    "        batch_companies = unique_companies[i:i+batch_size]\n",
    "        batch_data = data[data['Company_ID'].isin(batch_companies)]\n",
    "        \n",
    "        X_train = batch_data.drop(columns=[target_column, 'Company_ID', 'Date'])\n",
    "        y_train = batch_data[target_column]\n",
    "\n",
    "        # Preprocess and scale data\n",
    "        X_train = preprocess_data(X_train)\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_train_reduced = select_important_features(pd.DataFrame(X_train_scaled), y_train, num_features)\n",
    "\n",
    "        continue_training = True\n",
    "        while continue_training:\n",
    "            # LightGBM Training in parallel\n",
    "            train_lgbm = input(\"Do you want to train LightGBM? (y/n): \").lower() == 'y'\n",
    "            if train_lgbm:\n",
    "                num_estimators = int(input(\"Enter the number of estimators for LightGBM: \"))\n",
    "                learning_rate = float(input(\"Enter the learning rate for LightGBM: \"))\n",
    "                max_depth = int(input(\"Enter the max depth for LightGBM: \"))\n",
    "                num_leaves = int(input(\"Enter the number of leaves for LightGBM: \"))\n",
    "                feature_fraction = float(input(\"Enter the feature fraction for LightGBM (0.0 to 1.0): \"))\n",
    "                bagging_fraction = float(input(\"Enter the bagging fraction for LightGBM (0.0 to 1.0): \"))\n",
    "                bagging_freq = int(input(\"Enter the bagging frequency for LightGBM: \"))\n",
    "                lambda_l1 = float(input(\"Enter the L1 regularization for LightGBM: \"))\n",
    "                lambda_l2 = float(input(\"Enter the L2 regularization for LightGBM: \"))\n",
    "\n",
    "                # Add parameter set for parallel processing\n",
    "                params = {\n",
    "                    'X_train_reduced': X_train_reduced,\n",
    "                    'y_train': y_train,\n",
    "                    'num_estimators': num_estimators,\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'max_depth': max_depth,\n",
    "                    'num_leaves': num_leaves,\n",
    "                    'feature_fraction': feature_fraction,\n",
    "                    'bagging_fraction': bagging_fraction,\n",
    "                    'bagging_freq': bagging_freq,\n",
    "                    'lambda_l1': lambda_l1,\n",
    "                    'lambda_l2': lambda_l2\n",
    "                }\n",
    "                \n",
    "                # Train LightGBM models in parallel\n",
    "                parallel_lightgbm_training([params], X_train_reduced, y_train)\n",
    "\n",
    "            # Transformer Training with Mini-batch processing\n",
    "            train_transformer_model = input(\"Do you want to train the Transformer? (y/n): \").lower() == 'y'\n",
    "            transformer_model = None\n",
    "            if train_transformer_model:\n",
    "                epochs = int(input(\"Enter the number of epochs for Transformer: \"))\n",
    "                transformer_lr = float(input(\"Enter the learning rate for Transformer: \"))\n",
    "                \n",
    "                load_transformer = input(\"Do you want to load a saved Transformer model? (y/n): \").lower() == 'y'\n",
    "                if load_transformer:\n",
    "                    filename = input(\"Enter the filename of the saved Transformer model: \")\n",
    "                    transformer_model = load_model(filename, X_train_reduced.shape[1], device)\n",
    "                \n",
    "                # Mini-batch training for Transformer\n",
    "                transformer_model = train_transformer(\n",
    "                    X_train_reduced, y_train, \n",
    "                    X_train_reduced, y_train, \n",
    "                    X_train_reduced.shape[1], \n",
    "                    epochs, transformer_lr, model=transformer_model, batch_size=64, device=device\n",
    "                )\n",
    "                \n",
    "                save_transformer = input(\"Do you want to save this Transformer model? (y/n): \").lower() == 'y'\n",
    "                if save_transformer:\n",
    "                    transformer_filename = input(\"Enter the filename to save Transformer model: \")\n",
    "                    save_model(transformer_model, f\"{transformer_filename}.pth\")\n",
    "                    print(f'Transformer model saved as {transformer_filename}')\n",
    "\n",
    "            # Ask if the user wants to continue training the models\n",
    "            continue_training = input(\"Do you want to continue training the models? (y/n): \").lower() == 'y'\n",
    "\n",
    "# Main function to process the entire dataset\n",
    "def train_model_on_small_dataset(file_path, target_column='Close_x', batch_size=2, num_features=10):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    process_two_companies(df, target_column, batch_size, num_features)\n",
    "\n",
    "# Example usage\n",
    "file_path = 'final_cleaned.parquet'\n",
    "train_model_on_small_dataset(file_path, num_features=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0d6815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = 'final_cleaned.parquet'\n",
    "file= pd.read_parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec125854",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c4cf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values = file.isnull().sum()\n",
    "\n",
    "# Display the columns with their respective number of null values\n",
    "print(null_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20b4cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, display only columns that have null values\n",
    "null_columns = null_values[null_values > 0]\n",
    "print(null_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f080071",
   "metadata": {},
   "outputs": [],
   "source": [
    "Best trial: {'n_estimators': 24306, 'learning_rate': 0.049412489369297465, 'num_leaves': 65, 'max_depth': 11, 'min_child_weight': 2, 'subsample': 0.7486537063053206, 'colsample_bytree': 0.8938290344572073, 'lambda_l1': 5.552076717390757, 'lambda_l2': 5.664327006224713}\n",
    "Best RMSE: 1.1564605583932328\n",
    "Training was not successful.\n",
    "Best trial: {'n_estimators': 1896, 'learning_rate': 0.04036080448608824, 'num_leaves': 35, 'max_depth': 20, 'min_child_weight': 5, 'subsample': 0.6082873778110115, 'colsample_bytree': 0.6064640614372323, 'lambda_l1': 0.22322036512234522, 'lambda_l2': 6.288264467695819}\n",
    "Best RMSE: 1.1548257651807403\n",
    "Training was not successful.\n",
    "Best trial: {'n_estimators': 906, 'learning_rate': 0.04854017200771697, 'num_leaves': 73, 'max_depth': 20, 'min_child_weight': 1, 'subsample': 0.9405242535067422, 'colsample_bytree': 0.8092277269150399, 'lambda_l1': 1.409218395432035, 'lambda_l2': 6.813061948500454}\n",
    "Best RMSE: 1.1897049380084983\n",
    "Training was not successful.\n",
    "Best trial: {'n_estimators': 515, 'learning_rate': 0.04533535032250655, 'num_leaves': 85, 'max_depth': 6, 'min_child_weight': 4, 'subsample': 0.9920627684940627, 'colsample_bytree': 0.6199230615756138, 'lambda_l1': 5.053218586747487, 'lambda_l2': 7.2048827945514535}\n",
    "Best RMSE: 1.2306108760078551\n",
    "Training was not successful.\n",
    "Best trial: {'n_estimators': 518, 'learning_rate': 0.04350155281960924, 'num_leaves': 78, 'max_depth': 11, 'min_child_weight': 2, 'subsample': 0.6077376957652024, 'colsample_bytree': 0.6092134142649996, 'lambda_l1': 5.247855359976364, 'lambda_l2': 6.6878055740679}\n",
    "Best RMSE: 1.2026481037101264\n",
    "Training was not successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ce5641",
   "metadata": {},
   "outputs": [],
   "source": [
    "## hyperparameter tuning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import optuna\n",
    "from optuna.pruners import HyperbandPruner\n",
    "\n",
    "# Objective function for hyperparameter optimization\n",
    "def objective(trial, X_train, y_train):\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 30000, 35000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 31, 128),\n",
    "        'max_depth': trial.suggest_int('max_depth', -1, 20),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 5),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 0.01, 10.0),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 0.01, 10.0),\n",
    "    }\n",
    "\n",
    "    kf = KFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "    for train_index, valid_index in kf.split(X_train):\n",
    "        X_train_split, X_valid_split = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
    "        y_train_split, y_valid_split = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
    "\n",
    "        model = LGBMRegressor(**param)\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(\n",
    "            X_train_split, y_train_split,\n",
    "            eval_set=[(X_valid_split, y_valid_split)],\n",
    "            eval_metric='rmse',\n",
    "            callbacks=[optuna.integration.LightGBMPruningCallback(trial, 'rmse')]\n",
    "        )\n",
    "\n",
    "        # Predict and calculate RMSE\n",
    "        y_pred = model.predict(X_valid_split)\n",
    "        rmse = mean_squared_error(y_valid_split, y_pred, squared=False)\n",
    "\n",
    "        return rmse\n",
    "\n",
    "# Preprocessing function to handle Label Encoding for categorical data\n",
    "def preprocess_data(df):\n",
    "    # Convert object types to categorical\n",
    "    le = LabelEncoder()\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to train LightGBM with hyperparameter tuning and feature selection\n",
    "def train_lightgbm_with_tuning_and_feature_selection(file_path, target_column='Close_x', batch_size=10000, min_features=10):\n",
    "    print(\"Loading data...\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    df = pd.read_parquet(file_path, engine='pyarrow')\n",
    "\n",
    "    # Preprocess the dataset\n",
    "    df = preprocess_data(df)\n",
    "\n",
    "    # Select only two companies for modeling\n",
    "    selected_companies = df['Company_ID'].unique()[:2]\n",
    "    df = df[df['Company_ID'].isin(selected_companies)]\n",
    "\n",
    "    X_full = df.drop(columns=[target_column, 'Date'])\n",
    "    y_full = df[target_column]\n",
    "\n",
    "    # Optuna study with hyperparameter tuning\n",
    "    pruner = HyperbandPruner()\n",
    "    study = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "    study.optimize(lambda trial: objective(trial, X_full, y_full), n_trials=50)\n",
    "\n",
    "    # Output the best parameters and results\n",
    "    print(\"Best trial:\", study.best_trial.params)\n",
    "    print(\"Best RMSE:\", study.best_value)\n",
    "\n",
    "# Main function to train the model on two companies\n",
    "def train_model_on_two_companies(file_path, target_column='Close_x', min_features=10):\n",
    "    print(\"Training LightGBM on two companies with feature selection and hyperparameter tuning...\")\n",
    "    model = train_lightgbm_with_tuning_and_feature_selection(file_path, target_column, min_features=min_features)\n",
    "    if model is not None:\n",
    "        print(\"Training completed with the best model.\")\n",
    "    else:\n",
    "        print(\"Training was not successful.\")\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "file_path = 'final_outliers_2.parquet'\n",
    "train_model_on_two_companies(file_path, min_features=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4e13b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Summary of Results for Least Estimators:\n",
    "Companies_10_Least_Estimators: RMSE = 2.669920816356922\n",
    "Companies_50_Least_Estimators: RMSE = 26.897224035199706\n",
    "Companies_100_Least_Estimators: RMSE = 3.5614469055576667\n",
    "Companies_500_Least_Estimators: RMSE = 22.465645727070743"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c1d189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Preprocessing function to handle Label Encoding for categorical data\n",
    "def preprocess_data(df):\n",
    "    # Convert object types to categorical\n",
    "    le = LabelEncoder()\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to perform feature selection based on feature importance\n",
    "def feature_selection(X_train, y_train, min_features):\n",
    "    model = LGBMRegressor(n_estimators=100)  # Use a base model to get feature importance\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    feature_importances = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "    \n",
    "    # Select all features with non-zero importance, but ensure at least 'min_features'\n",
    "    top_features = feature_importances[feature_importances > 0].nlargest(max(min_features, (feature_importances > 0).sum())).index\n",
    "    \n",
    "    print(f\"Selected {len(top_features)} features for training.\")\n",
    "    \n",
    "    return top_features\n",
    "\n",
    "# Function to train LightGBM model after feature selection\n",
    "def train_lightgbm_with_feature_selection(X_train, y_train, params, selected_features):\n",
    "    X_train_selected = X_train[selected_features]\n",
    "    \n",
    "    kf = KFold(n_splits=2, shuffle=True, random_state=42)\n",
    "    rmse_list = []\n",
    "\n",
    "    for train_index, valid_index in kf.split(X_train_selected):\n",
    "        X_train_split, X_valid_split = X_train_selected.iloc[train_index], X_train_selected.iloc[valid_index]\n",
    "        y_train_split, y_valid_split = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
    "\n",
    "        model = LGBMRegressor(**params)\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(X_train_split, y_train_split, eval_set=[(X_valid_split, y_valid_split)], eval_metric='rmse')\n",
    "        \n",
    "        # Predict and calculate RMSE\n",
    "        y_pred = model.predict(X_valid_split)\n",
    "        rmse = mean_squared_error(y_valid_split, y_pred, squared=False)\n",
    "        rmse_list.append(rmse)\n",
    "    \n",
    "    return np.mean(rmse_list)\n",
    "\n",
    "# Function to train LightGBM on 500 random companies with dynamic feature selection\n",
    "def train_on_500_companies_with_dynamic_feature_selection(file_path, target_column='Close_x', min_features=10):\n",
    "    print(\"Loading data...\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    df = pd.read_parquet(file_path, engine='pyarrow')\n",
    "\n",
    "    # Preprocess the dataset\n",
    "    df = preprocess_data(df)\n",
    "\n",
    "    # Select 500 random companies\n",
    "    selected_companies = np.random.choice(df['Company_ID'].unique(), size=500, replace=False)\n",
    "    df = df[df['Company_ID'].isin(selected_companies)]\n",
    "\n",
    "    X_full = df.drop(columns=[target_column, 'Date'])\n",
    "    y_full = df[target_column]\n",
    "\n",
    "    # Parameter set with the highest number of estimators\n",
    "    best_params_high_n_estimators = {\n",
    "        'n_estimators': 2000,\n",
    "        'learning_rate': 0.049412489369297465,\n",
    "        'num_leaves': 65,\n",
    "        'max_depth': 11,\n",
    "        'min_child_weight': 2,\n",
    "        'subsample': 0.7486537063053206,\n",
    "        'colsample_bytree': 0.8938290344572073,\n",
    "        'lambda_l1': 5.552076717390757,\n",
    "        'lambda_l2': 5.664327006224713\n",
    "    }\n",
    "\n",
    "    # Select features dynamically based on feature importance\n",
    "    selected_features = feature_selection(X_full, y_full, min_features)\n",
    "    \n",
    "    print(f\"\\n--- Training for 100 random companies with {len(selected_features)} selected features ---\")\n",
    "    rmse = train_lightgbm_with_feature_selection(X_full, y_full, best_params_high_n_estimators, selected_features)\n",
    "    \n",
    "    print(\"\\nFinal RMSE for 100 companies:\", rmse)\n",
    "\n",
    "# Example usage\n",
    "file_path = 'final_outliers_2.parquet'\n",
    "train_on_500_companies_with_dynamic_feature_selection(file_path, min_features=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
